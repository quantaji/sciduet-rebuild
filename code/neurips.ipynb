{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no slides for 2020\n",
    "years = [2018, 2019, 2021, 2022] # only these years are possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_store_dir = '../initial_data/stage_1/extracted_urls'\n",
    "if not os.path.exists(url_store_dir):\n",
    "    os.makedirs(url_store_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    schedule_url = 'https://nips.cc/Conferences/{year}/Schedule'.format(year=year)\n",
    "\n",
    "    r = requests.get(schedule_url)\n",
    "    soup = BeautifulSoup(r.content, \"lxml\")\n",
    "    filtered = soup.find_all('div', string=['Oral', 'Poster', 'Spotlight'])  # leave only papers remain\n",
    "\n",
    "    pair_all = {'titles': [], 'slides': [], 'papers': []}\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    paper_list_url = 'https://papers.nips.cc/paper/{year}'.format(year=year)\n",
    "    r_tmp = requests.get(paper_list_url)\n",
    "    paper_list = BeautifulSoup(r_tmp.content, \"lxml\")\n",
    "\n",
    "    for item in filtered:\n",
    "\n",
    "        main_item = item.parent\n",
    "\n",
    "        # find slides url now\n",
    "        slide_url = None\n",
    "        slide_item = main_item.find('a', title='Slides')\n",
    "        if not slide_item:\n",
    "            continue\n",
    "        else:\n",
    "            temp = slide_item['href']\n",
    "            if temp[-4:] != '.pdf':\n",
    "                continue\n",
    "            else:\n",
    "                slide_url = temp\n",
    "\n",
    "                if slide_url[:6] == '/media':\n",
    "                    slide_url = 'nips.cc' + slide_url\n",
    "\n",
    "        # title\n",
    "        title = main_item.find('div', class_=\"maincardBody\").string\n",
    "\n",
    "        # find paper url now\n",
    "        paper_url = None\n",
    "        paper_link = main_item.find_all('a', title=['Paper', 'OpenReview'])\n",
    "        if paper_link:\n",
    "\n",
    "            if paper_link[0]['title'] == 'Paper':  # 2018, 2019\n",
    "\n",
    "                r = requests.get(paper_link[0]['href'])\n",
    "                tmpsoup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "                paper_link_tmp = tmpsoup.find(\"a\", string='Paper')['href']\n",
    "                paper_url = 'proceedings.neurips.cc' + paper_link_tmp\n",
    "            else:  # 2021 2022\n",
    "                r = requests.get(paper_link[0]['href'])\n",
    "                tmpsoup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "                download_item = tmpsoup.find('a', class_='note_content_pdf')\n",
    "                if download_item is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    paper_url = 'openreview.net' + download_item['href']\n",
    "        else:\n",
    "            # no paper in this page, search from paper list page\n",
    "\n",
    "            if '$' in title:\n",
    "                continue  # no math expression in title\n",
    "\n",
    "            # pattern = r'(.)*' + re.escape(title) + r'(.)*'\n",
    "            tmp = paper_list.find('a', string=re.compile(title))\n",
    "\n",
    "            if not tmp:\n",
    "                continue\n",
    "\n",
    "            r = requests.get('https://papers.nips.cc' + tmp['href'])\n",
    "            tmpsoup = BeautifulSoup(r.content, 'lxml')\n",
    "\n",
    "            paper_link_tmp = tmpsoup.find(\"a\", string='Paper')['href']\n",
    "            paper_url = 'proceedings.neurips.cc' + paper_link_tmp\n",
    "\n",
    "        slide_url = slide_url.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "        paper_url = paper_url.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "\n",
    "        pair_all['titles'].append(title)\n",
    "        pair_all['slides'].append(slide_url)\n",
    "        pair_all['papers'].append(paper_url)\n",
    "\n",
    "        print(index, title, paper_url, slide_url, sep=' ')\n",
    "        index += 1\n",
    "\n",
    "    pd.DataFrame(pair_all).to_csv(os.path.join(url_store_dir, 'neurips_{year}.csv'.format(year=year)))\n",
    "    print('Total count: ', index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciduet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
